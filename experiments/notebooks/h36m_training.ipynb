{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/caleml/main-pe/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datasets.h36m import Human36M\n",
    "from data.utils.data_utils import TEST_MODE, TRAIN_MODE, VALID_MODE\n",
    "from data.loader import BatchLoader\n",
    "\n",
    "from experiments.common import exp_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import blocks\n",
    "from model import layers\n",
    "from model import losses\n",
    "from model import config\n",
    "from model import callbacks\n",
    "from model.utils import pose_format, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.networks.multi_branch_model import MultiBranchModel\n",
    "from model.networks.mbm_vgg import MultiBranchVGGModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.networks.mbm_reduced import MultiBranchReduced, MultiBranchStopped\n",
    "from model.networks.cycle_reduced import CycleReduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local loading\n",
    "h36m_path = '/home/caleml/datasets/h36m'\n",
    "h36m = Human36M(h36m_path, dataconf=config.human36m_dataconf, poselayout=pose_format.pa17j3d, topology='frames') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'exp_type': 'hybrid_vgg_TEST_NB',\n",
    "    'dim': 3,\n",
    "    'n_joints': 17,\n",
    "    'pose_blocks': 2,\n",
    "    'dataset_name': 'h36m',\n",
    "    'batch_size': 8,\n",
    "    'n_epochs': 60\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'exp_type': 'hybrid_reduced128_cycle_TEST_NB',\n",
    "    'dim': 3,\n",
    "    'n_joints': 17,\n",
    "    'pose_blocks': 2,\n",
    "    'dataset_name': 'h36m',\n",
    "    'batch_size': 8,\n",
    "    'n_epochs': 60\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conducting experiment for 60 epochs and 2 blocks in folder /home/caleml/pe_experiments/exp_20190517_1923_hybrid_reduced128_cycle_TEST_NB__2b_bs8\n"
     ]
    }
   ],
   "source": [
    "model_folder = exp_init(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "h36m_val = BatchLoader(\n",
    "    h36m, \n",
    "    ['frame'], \n",
    "    ['pose_w', 'pose_uvd', 'afmat', 'camera'], \n",
    "    VALID_MODE, \n",
    "    batch_size=h36m.get_length(VALID_MODE), \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.printcn(log.OKBLUE, 'Preloading Human3.6M validation samples...')\n",
    "[x_val], [pw_val, puvd_val, afmat_val, scam_val] = h36m_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = callbacks.H36MEvalCallback(conf['pose_blocks'], x_val, pw_val, afmat_val, puvd_val[:,0,2], scam_val, logdir=model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG with action for phony placeholders\n",
    "data_tr_h36m = BatchLoader(\n",
    "        h36m, \n",
    "        ['frame'], \n",
    "        ['action'] * 3 + ['pose'] * conf['pose_blocks'],\n",
    "        TRAIN_MODE, \n",
    "        batch_size=conf['batch_size'],\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG loader with phony keyword\n",
    "data_tr_h36m = BatchLoader(\n",
    "        h36m, \n",
    "        ['frame'], \n",
    "        ['phony'] * 3 + ['pose'] * conf['pose_blocks'],\n",
    "        TRAIN_MODE, \n",
    "        batch_size=conf['batch_size'],\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG loader with specific phony sizes\n",
    "data_tr_h36m = BatchLoader(\n",
    "        h36m, \n",
    "        ['frame'], \n",
    "        ['phony_2_b_256_256_64', 'phony_2_b_128_128_128', 'phony_2_b_64_64_256'] + ['pose'] * conf['pose_blocks'],\n",
    "        TRAIN_MODE, \n",
    "        batch_size=conf['batch_size'],\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_dict ['frame', 'pose', 'pose'], allkeys ['frame', 'frame', 'pose', 'pose']\n"
     ]
    }
   ],
   "source": [
    "# classical i_hat + pose format\n",
    "data_tr_h36m = BatchLoader(\n",
    "        h36m, \n",
    "        ['frame'], \n",
    "        ['frame'] + ['pose'] * conf['pose_blocks'],\n",
    "        TRAIN_MODE, \n",
    "        batch_size=conf['batch_size'],\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classical + info for train eval\n",
    "data_tr_h36m = BatchLoader(\n",
    "        h36m, \n",
    "        ['frame'], \n",
    "        ['frame'] + ['pose'] * conf['pose_blocks'] + ['pose_w', 'afmat', 'pose_uvd', 'camera'],\n",
    "        TRAIN_MODE, \n",
    "        batch_size=conf['batch_size'],\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_dict ['frame', 'pose', 'pose', 'phony', 'phony', 'phony'], allkeys ['frame', 'frame', 'pose', 'pose']\n"
     ]
    }
   ],
   "source": [
    "# cycle\n",
    "data_tr_h36m = BatchLoader(\n",
    "        h36m, \n",
    "        ['frame'], \n",
    "        ['frame'] + ['pose'] * conf['pose_blocks'] + ['phony', 'phony', 'phony'],\n",
    "        TRAIN_MODE, \n",
    "        batch_size=conf['batch_size'],\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical multi branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiBranchModel(dim=conf['dim'], n_joints=conf['n_joints'], nb_pose_blocks=conf['pose_blocks'])\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add_callback(eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data_tr_h36m, steps_per_epoch=len(data_tr_h36m), model_folder=model_folder, n_epochs=conf['n_epochs'], cb_list=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short test for cb\n",
    "model.train(data_tr_h36m, steps_per_epoch=10, model_folder=model_folder, n_epochs=conf['n_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG multi branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiBranchVGGModel(dim=conf['dim'], n_joints=conf['n_joints'], nb_pose_blocks=conf['pose_blocks'])\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data_tr_h36m, steps_per_epoch=len(data_tr_h36m), model_folder=model_folder, n_epochs=conf['n_epochs'], cb_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced multi branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "zp_depth 128\n",
      "pose shape (?, 17, 3), vis shape (?, 17, 1), concat shape (?, 17, 4)\n",
      "pose shape (?, 17, 3), vis shape (?, 17, 1), concat shape (?, 17, 4)\n",
      "Last H shape Tensor(\"batch_normalization_37/cond/Merge:0\", shape=(?, 32, 32, 576), dtype=float32)\n",
      "\u001b[95mBuild E_a 1.8262348175048828, build E_p 6.074084758758545, decoder D 0.8654658794403076\u001b[0m\n",
      "\u001b[95mInput shape (?, 256, 256, 3)\u001b[0m\n",
      "\u001b[95mShape z_a (?, 16, 16, 128), shape z_p (?, 16, 16, 128)\u001b[0m\n",
      "\u001b[95mOutputs shape [(None, 256, 256, 3), (None, 17, 4), (None, 17, 4)]\u001b[0m\n",
      "rec y_pred shape (?, 256, 256, 3)\n",
      "\u001b[95mFinal model summary\u001b[0m\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "appearance_model (Model)        (None, 16, 16, 128)  690368      image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pose_model (Model)              [(None, 17, 4), (Non 5103826     image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_concat (Concatenate)          (None, 16, 16, 256)  0           appearance_model[1][0]           \n",
      "                                                                 pose_model[1][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 256, 256, 3)  582843      z_concat[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,377,037\n",
      "Trainable params: 6,311,343\n",
      "Non-trainable params: 65,694\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from model.networks.mbm_reduced import MultiBranchReduced\n",
    "model = MultiBranchReduced(dim=conf['dim'], n_joints=conf['n_joints'], nb_pose_blocks=conf['pose_blocks'])\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_folder = os.environ['HOME'] + '/pe_experiments/tensorboard/' + model_folder.split('/')[-1]\n",
    "print('Tensorboard log folder %s' % logs_folder)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(logs_folder, 'tensorboard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = callbacks.H36MEvalCallback(conf['pose_blocks'], x_val, pw_val, afmat_val, puvd_val[:,0,2], scam_val, pose_only=False, logdir=model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_list.append(tensorboard)\n",
    "cb_list.append(eval_callback)\n",
    "# cb_list.append(LearningRateScheduler(lr_scheduler))\n",
    "cb_list.append(callbacks.SaveModel(model_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with CB\n",
    "model.train(data_tr_h36m, steps_per_epoch=10, model_folder=model_folder, n_epochs=conf['n_epochs'], cb_list=cb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 0 callbacks\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 6 arrays: [array([[[[-0.09654069, -0.36644757, -0.39916641],\n         [-0.10441631, -0.37434846, -0.40718222],\n         [-0.10441631, -0.37434846, -0.40718222],\n         ...,\n         [ 0.11657727, -0.17794675,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-43365c7a3e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train without CB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tr_h36m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/main-pe/model/networks/__init__.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_tr, steps_per_epoch, model_folder, n_epochs, cb_list)\u001b[0m\n\u001b[1;32m     44\u001b[0m                                  \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                  \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                                  initial_epoch=0)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/lib/python3.4/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 1928\u001b[0;31m         x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1152\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    291\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m       raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 6 arrays: [array([[[[-0.09654069, -0.36644757, -0.39916641],\n         [-0.10441631, -0.37434846, -0.40718222],\n         [-0.10441631, -0.37434846, -0.40718222],\n         ...,\n         [ 0.11657727, -0.17794675,..."
     ]
    }
   ],
   "source": [
    "# train without CB\n",
    "model.train(data_tr_h36m, steps_per_epoch=10, model_folder=model_folder, n_epochs=2, cb_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiBranchStopped(dim=conf['dim'], n_joints=conf['n_joints'], nb_pose_blocks=conf['pose_blocks'])\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data_tr_h36m, steps_per_epoch=100, model_folder=model_folder, n_epochs=conf['n_epochs'], cb_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "zp_depth 128\n",
      "pose shape (?, 17, 3), vis shape (?, 17, 1), concat shape (?, 17, 4)\n",
      "pose shape (?, 17, 3), vis shape (?, 17, 1), concat shape (?, 17, 4)\n",
      "Last H shape Tensor(\"batch_normalization_83/cond/Merge:0\", shape=(?, 32, 32, 576), dtype=float32)\n",
      "\u001b[95mBuild E_a 2.5040910243988037, build E_p 15.259744644165039, decoder D 0.567155122756958\u001b[0m\n",
      "\u001b[95mInput shape (?, 256, 256, 3)\u001b[0m\n",
      "\u001b[95mShape z_a (?, 16, 16, 128), shape z_p (?, 16, 16, 128)\u001b[0m\n",
      "z_p sums Tensor(\"Sum:0\", shape=(?,), dtype=float32)\n",
      "mixed z_p sums Tensor(\"Sum_1:0\", shape=(?,), dtype=float32)\n",
      "Outputs shape [(None, 256, 256, 3), (None, 17, 4), (None, 17, 4), (None, 16, 16, 256), (None, 16, 16, 256), (None, 256, 256, 3)]\n",
      "rec y_pred shape (?, 256, 256, 3)\n",
      "cycle y_pred shape (?, 16, 16, 256)\n",
      "depth 256\n",
      "z_x shape (?, 16, 16, 128), z_x_cycle shape (?, 16, 16, 128)\n",
      "cycle y_pred shape (?, 16, 16, 256)\n",
      "depth 256\n",
      "z_x shape (?, 16, 16, 128), z_x_cycle shape (?, 16, 16, 128)\n",
      "\u001b[95mFinal model summary\u001b[0m\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pose_model (Model)              [(None, 17, 4), (Non 5103826     image_input[0][0]                \n",
      "                                                                 i_hat_mixed[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "appearance_model (Model)        (None, 16, 16, 128)  690368      image_input[0][0]                \n",
      "                                                                 i_hat_mixed[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "shuffled_zp (Lambda)            (None, 16, 16, 128)  0           pose_model[1][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_concat (Concatenate)          (None, 16, 16, 256)  0           appearance_model[1][0]           \n",
      "                                                                 pose_model[1][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "concat_shuffle (Concatenate)    (None, 16, 16, 256)  0           appearance_model[1][0]           \n",
      "                                                                 shuffled_zp[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 256, 256, 3)  582843      z_concat[0][0]                   \n",
      "                                                                 concat_shuffle[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "i_hat_mixed (Lambda)            (None, 256, 256, 3)  0           decoder[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "i_hat (Lambda)                  (None, 256, 256, 3)  0           decoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cycle_za_concat (Concatenate)   (None, 16, 16, 256)  0           appearance_model[1][0]           \n",
      "                                                                 appearance_model[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "cycle_zp_concat (Concatenate)   (None, 16, 16, 256)  0           pose_model[1][2]                 \n",
      "                                                                 pose_model[2][2]                 \n",
      "==================================================================================================\n",
      "Total params: 6,377,037\n",
      "Trainable params: 6,311,343\n",
      "Non-trainable params: 65,694\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from model.networks.cycle_reduced import CycleReduced\n",
    "model = CycleReduced(dim=conf['dim'], n_joints=conf['n_joints'], nb_pose_blocks=conf['pose_blocks'])\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 0 callbacks\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 24s 2s/step - loss: 48.1015 - i_hat_loss: 29.0175 - pose_model_loss: 4.6823 - cycle_za_concat_loss: 3.2992 - cycle_zp_concat_loss: 6.4033 - i_hat_mixed_loss: 0.0000e+00\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 3s 277ms/step - loss: 30.8937 - i_hat_loss: 22.0040 - pose_model_loss: 3.0026 - cycle_za_concat_loss: 1.2692 - cycle_zp_concat_loss: 1.6417 - i_hat_mixed_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model.train(data_tr_h36m, steps_per_epoch=10, model_folder=model_folder, n_epochs=2, cb_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG debug stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layers = [1,3,4,6,7]\n",
    "for i in output_layers:\n",
    "    print(vgg_model.layers[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1_1,conv2_1,conv3_1,pool1,pool2\n",
    "# from https://discuss.pytorch.org/t/how-to-use-vgg-19-network-to-estimate-perceptual-loss/9981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'phony_2_b_256_256_64'\n",
    "batch_size = 24\n",
    "b = [int(elt) if elt.lower() != 'b' else batch_size for elt in a.split('_')[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(a, b, c):\n",
    "    \n",
    "    def loss(y_true, y_pred):\n",
    "        return y_true + y_pred + a + b + c\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = foo(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(loss))\n",
    "loss(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "annot_file = os.path.join(h36m_path, 'annotations.mat')\n",
    "mat = sio.loadmat(annot_file, struct_as_record=False, squeeze_me=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [mat['sequences_te'], mat['sequences_tr'], mat['sequences_val']]\n",
    "action_labels = mat['action_labels']\n",
    "joint_labels = mat['joint_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(joint_labels)\n",
    "joint_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human 3.6 -> PA17J\n",
    "joint_labels[[0, 12, 13, 15, 25, 17, 26, 18, 27, 19, 1, 6, 2, 7, 3, 8, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sequences[1]))\n",
    "sequences[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3], [4,5,6]])\n",
    "b = np.reshape(a, 6)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
