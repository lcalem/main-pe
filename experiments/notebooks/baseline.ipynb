{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/caleml/main-pe/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D, UpSampling2D, Convolution2D, Activation, BatchNormalization, Reshape\n",
    "from tensorflow.keras.layers import Permute, add, concatenate\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datasets.mpii import MpiiSinglePerson\n",
    "from data.utils.data_utils import TEST_MODE, TRAIN_MODE, VALID_MODE\n",
    "from data.loader import BatchLoader\n",
    "\n",
    "from model import blocks\n",
    "from model import layers\n",
    "from model import losses\n",
    "from model import config\n",
    "from model import callbacks\n",
    "from model.models import BaseModel, AppearanceModel\n",
    "from model.utils import pose_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseModel(object):\n",
    "    \n",
    "    def __init__(self, input_tensor, n_joints, n_blocks, kernel_size):\n",
    "        self.n_joints = n_joints\n",
    "        self.n_blocks = n_blocks\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.n_heatmaps = self.n_joints   # this seems silly but we will augment with context later\n",
    "        \n",
    "        self.build(input_tensor)\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "        \n",
    "    def build(self, inp):\n",
    "        '''\n",
    "        1. stem\n",
    "        2. stacking the blocks\n",
    "        '''\n",
    "        \n",
    "        outputs = list() \n",
    "        x = self.stem(inp)\n",
    "        \n",
    "        # static layers\n",
    "        num_rows, num_cols, num_filters = x.get_shape().as_list()[1:]\n",
    "        # print(\"num rows %s, num cols %s, num filters %s\" % (num_rows, num_cols, num_filters))\n",
    "        pose_input_shape = (num_rows, num_cols, self.n_joints)   # (32, 32, 16)\n",
    "        self.pose_softargmax_model = self.build_softargmax_model(pose_input_shape)\n",
    "        self.joint_visibility_model = self.build_visibility_model(pose_input_shape)\n",
    "        \n",
    "        # hourglass blocks\n",
    "        for i_block in range(self.n_blocks):\n",
    "            \n",
    "            block_shape = x.get_shape().as_list()[1:]\n",
    "            x = self.reception_block(x, name='rBlock%d' % (i_block + 1))\n",
    "            \n",
    "            identity_map = x\n",
    "            x = self.sepconv_block(x, name='SepConv%d' % (i_block + 1))\n",
    "            h = self.pose_block(x, name='RegMap%d' % (i_block + 1))\n",
    "            \n",
    "            pose, visible = self.pose_regression_2d(h, name='PoseReg%s' % (i_block + 1))\n",
    "            pose_vis = concatenate([pose, visible], axis=-1)\n",
    "            print(\"pose shape %s, vis shape %s, concat shape %s\" % (str(pose.shape), str(visible.shape), str(pose_vis.shape)))\n",
    "            \n",
    "            outputs.append(pose_vis)\n",
    "\n",
    "            if i_block < self.n_blocks - 1:\n",
    "                h = self.fremap_block(h, block_shape[-1], name='fReMap%d' % (i_block + 1))\n",
    "                x = add([identity_map, x, h])\n",
    "                \n",
    "        self._model = Model(inputs=inp, outputs=outputs)\n",
    "        \n",
    "    def stem(self, inp):\n",
    "        '''\n",
    "        inception v4 stem\n",
    "        \n",
    "        input: 256 x 256 x 3\n",
    "        output: 32 x 32 x 576\n",
    "        '''\n",
    "        xi = Input(shape=inp.get_shape().as_list()[1:]) # 256 x 256 x 3\n",
    "\n",
    "        x = layers.conv_bn_act(xi, 32, (3, 3), strides=(2, 2))\n",
    "        x = layers.conv_bn_act(x, 32, (3, 3))\n",
    "        x = layers.conv_bn_act(x, 64, (3, 3))\n",
    "\n",
    "        a = layers.conv_bn_act(x, 96, (3, 3), strides=(2, 2))\n",
    "        b = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "        x = concatenate([a, b])\n",
    "\n",
    "        a = layers.conv_bn_act(x, 64, (1, 1))\n",
    "        a = layers.conv_bn(a, 96, (3, 3))\n",
    "        b = layers.conv_bn_act(x, 64, (1, 1))\n",
    "        b = layers.conv_bn_act(b, 64, (5, 1))\n",
    "        b = layers.conv_bn_act(b, 64, (1, 5))\n",
    "        b = layers.conv_bn(b, 96, (3, 3))\n",
    "        x = concatenate([a, b])\n",
    "\n",
    "        a = layers.act_conv_bn(x, 192, (3, 3), strides=(2, 2))\n",
    "        b = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "        x = concatenate([a, b])\n",
    "\n",
    "        x = blocks.sepconv_residual(x, 3*192, name='sepconv1')\n",
    "\n",
    "        model = Model(xi, x, name='Stem')\n",
    "        x = model(inp)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build_softargmax_model(self, input_shape):\n",
    "        '''\n",
    "        Static model for soft argmax\n",
    "        '''\n",
    "\n",
    "        inp = Input(shape=input_shape)\n",
    "        x = layers.act_channel_softmax(inp)\n",
    "\n",
    "        x_x = lin_interpolation_2d(x, dim=0)\n",
    "        x_y = lin_interpolation_2d(x, dim=1)\n",
    "        pose = concatenate([x_x, x_y])\n",
    "\n",
    "        model = Model(inputs=inp, outputs=pose)\n",
    "        model.trainable = False\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_visibility_model(self, input_shape):\n",
    "        '''\n",
    "        Static model for joint visibility\n",
    "        '''\n",
    "        num_rows, num_cols = input_shape[0:2]\n",
    "        inp = Input(shape=input_shape)\n",
    "\n",
    "        x = MaxPooling2D((num_rows, num_cols))(inp)\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "        x = Lambda(lambda x: tf.squeeze(x, axis=1))(x)\n",
    "        x = Lambda(lambda x: tf.squeeze(x, axis=1))(x)\n",
    "        x = Lambda(lambda x: tf.expand_dims(x, axis=-1))(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "        return model\n",
    "        \n",
    "    def reception_block(self, inp, name):\n",
    "        '''\n",
    "        each pose block starts with a reception block\n",
    "        it is u-shaped and relies on separable convolutions\n",
    "        \n",
    "        inp ------------------------- a (SR 576) -------------------- + -- out\n",
    "          |                                                           |\n",
    "          |                                                           |\n",
    "          MP --- C 288 -- SR 288 ---- b (SR 288) ---- + -- SR 576 -- US\n",
    "                            |                         |\n",
    "                            |                         |\n",
    "                            MP --- SR -- SR -- SR --- US     <- all 288 channels\n",
    "                            \n",
    "          \n",
    "        SR: Sepconv Residual (all 5x5)\n",
    "        C: Conv (1x1)\n",
    "        MP: Max Pooling (2x2 with stride 2x2)\n",
    "        US: UpSampling (2x2)\n",
    "        \n",
    "        input: 32 x 32 x 576\n",
    "        output: 32 x 32 x 576\n",
    "        '''\n",
    "        ksize = self.kernel_size\n",
    "        \n",
    "        input_shape = inp.get_shape().as_list()[1:]\n",
    "        size = int(input_shape[-1])\n",
    "\n",
    "        # first branch\n",
    "        xi = Input(shape=input_shape)\n",
    "        a = blocks.sepconv_residual(xi, size, name='sepconv_l1', kernel_size=ksize)\n",
    "\n",
    "        # second branch\n",
    "        low1 = MaxPooling2D((2, 2))(xi)\n",
    "        low1 = layers.act_conv_bn(low1, int(size/2), (1, 1))\n",
    "        low1 = blocks.sepconv_residual(low1, int(size/2), name='sepconv_l2_1', kernel_size=ksize)\n",
    "        b = blocks.sepconv_residual(low1, int(size/2), name='sepconv_l2_2', kernel_size=ksize)\n",
    "\n",
    "        # third branch\n",
    "        c = MaxPooling2D((2, 2))(low1)\n",
    "        c = blocks.sepconv_residual(c, int(size/2), name='sepconv_l3_1', kernel_size=ksize)\n",
    "        c = blocks.sepconv_residual(c, int(size/2), name='sepconv_l3_2', kernel_size=ksize)\n",
    "        c = blocks.sepconv_residual(c, int(size/2), name='sepconv_l3_3', kernel_size=ksize)\n",
    "        c = UpSampling2D((2, 2))(c)\n",
    "\n",
    "        # merge second and third branches\n",
    "        b = add([b, c])\n",
    "        b = blocks.sepconv_residual(b, size, name='sepconv_l2_3', kernel_size=ksize)\n",
    "        b = UpSampling2D((2, 2))(b)\n",
    "        \n",
    "        # merge first and second branches\n",
    "        x = add([a, b])\n",
    "        model = Model(inputs=xi, outputs=x, name=name)\n",
    "\n",
    "        return model(inp)\n",
    "    \n",
    "    def sepconv_block(self, inp, name):\n",
    "        '''\n",
    "        Separable convolution\n",
    "        '''\n",
    "        input_shape = inp.get_shape().as_list()[1:]\n",
    "\n",
    "        xi = Input(shape=input_shape)\n",
    "        x = layers.separable_act_conv_bn(xi, input_shape[-1], self.kernel_size)\n",
    "\n",
    "        model = Model(inputs=xi, outputs=x, name=name)\n",
    "\n",
    "        return model(inp)\n",
    "        \n",
    "    def pose_block(self, inp, name):\n",
    "        '''\n",
    "        input: 32 x 32 x 576\n",
    "        output: 32 x 32 x 16 (number of heatmaps)\n",
    "        '''\n",
    "        input_shape = inp.get_shape().as_list()[1:]\n",
    "\n",
    "        xi = Input(shape=input_shape)\n",
    "        x = layers.act_conv(xi, self.n_heatmaps, (1, 1))\n",
    "\n",
    "        model = Model(inputs=xi, outputs=x, name=name)\n",
    "\n",
    "        return model(inp)\n",
    "    \n",
    "    def pose_regression_2d(self, heatmaps, name):\n",
    "        '''\n",
    "        soft argmax to get the pose from the heatmaps\n",
    "        joint prob model to get the joint visibility probability\n",
    "        \n",
    "        input: 32 x 32 x 16 (number of joints)\n",
    "        output: \n",
    "        - pose (None, 16, 2)\n",
    "        - visibility (None, 16, 1)\n",
    "        '''\n",
    "        pose = self.pose_softargmax_model(heatmaps)\n",
    "        visibility = self.joint_visibility_model(heatmaps)\n",
    "        \n",
    "        return pose, visibility\n",
    "    \n",
    "    def fremap_block(self, inp, num_filters, name=None):\n",
    "        input_shape = inp.get_shape().as_list()[1:]\n",
    "\n",
    "        xi = Input(shape=input_shape)\n",
    "        x = layers.act_conv_bn(xi, num_filters, (1, 1))\n",
    "\n",
    "        model = Model(inputs=xi, outputs=x, name=name)\n",
    "\n",
    "        return model(inp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from model.utils import math\n",
    "\n",
    "def lin_interpolation_2d(inp, dim):\n",
    "    num_rows, num_cols, num_filters = inp.get_shape().as_list()[1:]\n",
    "    conv = SeparableConv2D(num_filters, (num_rows, num_cols), use_bias=False)\n",
    "    x = conv(inp)\n",
    "\n",
    "    w = conv.get_weights()\n",
    "    w[0].fill(0)\n",
    "    w[1].fill(0)\n",
    "    linspace = math.linspace_2d(num_rows, num_cols, dim=dim)\n",
    "\n",
    "    for i in range(num_filters):\n",
    "        w[0][:,:, i, 0] = linspace[:,:]\n",
    "        w[1][0, 0, i, i] = 1.\n",
    "\n",
    "    conv.set_weights(w)\n",
    "    conv.trainable = False\n",
    "    \n",
    "    x = Lambda(lambda x: tf.squeeze(x, axis=1))(x)\n",
    "    x = Lambda(lambda x: tf.squeeze(x, axis=1))(x)\n",
    "    x = Lambda(lambda x: tf.expand_dims(x, axis=-1))(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = SeparableConv2D(16, (32, 32), use_bias=False)\n",
    "w = conv.get_weights()\n",
    "print(len(w))\n",
    "print(\"%s %s %s\" % (len(w), w[0].shape, w[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(object):\n",
    "    \n",
    "    def __init__(self, input_tensor):\n",
    "        \n",
    "        self.build(input_tensor)\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    def build(self, inp):\n",
    "        z_a = Input(shape=inp.get_shape().as_list()[1:])  # for now, only the z_a part (8 x 8 x 2048)\n",
    "        \n",
    "        up = layers.up(z_a)  # 16 x 16\n",
    "        up = layers.conv_bn_act(up, 512, (3, 3))\n",
    "        up = layers.conv_bn_act(up, 512, (3, 3))\n",
    "        up = layers.conv_bn_act(up, 512, (3, 3))\n",
    "        \n",
    "        up = layers.up(up)  # 32 x 32\n",
    "        up = layers.conv_bn_act(up, 512, (3, 3))\n",
    "        up = layers.conv_bn_act(up, 512, (3, 3))\n",
    "        up = layers.conv_bn_act(up, 256, (3, 3))\n",
    "        \n",
    "        up = layers.up(up)  # 64 x 64\n",
    "        up = layers.conv_bn_act(up, 256, (3, 3))\n",
    "        up = layers.conv_bn_act(up, 256, (3, 3))\n",
    "        up = layers.conv_bn_act(up, 128, (3, 3))\n",
    "        \n",
    "        up = layers.up(up)  # 128 x 128\n",
    "        up = layers.conv_bn_act(up, 128, (3, 3))\n",
    "        up = layers.conv_bn_act(up, 64, (3, 3))\n",
    "        \n",
    "        up = layers.up(up)  # 256 x 256\n",
    "        up = layers.conv_bn_act(up, 3, (3, 3))\n",
    "        up = layers.conv_bn(up, 3, (1, 1))   # 3 channels, output shape of this should be (None, 3, 256, 256)\n",
    "            \n",
    "        # TODO: should we permute here or have the input formatted with channels first?\n",
    "        # perm = Permute((1, 2))(up)\n",
    "        # i_hat = Permute((2, 3))(perm)\n",
    "        i_hat = up\n",
    "        \n",
    "        self._model = Model(inputs=z_a, outputs=i_hat, name='decoder')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchModel(BaseModel):\n",
    "    '''\n",
    "    2 branch model :\n",
    "    - appearance (z_a)\n",
    "    - pose (z_p)\n",
    "    One common decoder to recreate the image\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_joints=16, nb_pose_blocks=8, reception_kernel_size=(5,5)):\n",
    "        self.n_joints = n_joints\n",
    "        self.n_blocks = nb_pose_blocks\n",
    "        self.reception_kernel_size = reception_kernel_size\n",
    "        \n",
    "        BaseModel.__init__(self)\n",
    "        \n",
    "    def build(self):\n",
    "        inp = Input(shape=self.input_shape)\n",
    "        \n",
    "        # encoders\n",
    "        time_1 = time.time()\n",
    "        z_a = self.appearance_encoder(inp)\n",
    "        time_2 = time.time()\n",
    "        z_p = self.pose_encoder(inp)\n",
    "        time_3 = time.time()\n",
    "        \n",
    "        print(\"Build E_a %s, build E_p %s\" % (time_2 - time_1, time_3 - time_2))\n",
    "        print(type(z_a), type(z_p))\n",
    "        print(\"Shape z_a %s\" % str(z_a.shape))\n",
    "        \n",
    "        # decoder\n",
    "        concat = self.concat(z_a, z_p)\n",
    "        print(\"Shape concat %s\" % str(concat.shape))\n",
    "        i_hat = self.decoder(concat)\n",
    "        \n",
    "        outputs = [i_hat]\n",
    "        outputs.extend(z_p)\n",
    "        self.model = Model(inputs=inp, outputs=outputs)\n",
    "        print(\"Outputs shape %s\" % self.model.output_shape)\n",
    "       \n",
    "        ploss = [pose_loss()] * len(z_p)\n",
    "        losses = [reconstruction_loss()]\n",
    "        losses.extend(ploss)\n",
    "        # loss = mean_squared_error\n",
    "        self.model.compile(loss=losses, optimizer=RMSprop(lr=self.start_lr))\n",
    "        self.model.summary()\n",
    "        \n",
    "    def appearance_encoder(self, inp):\n",
    "        '''\n",
    "        resnet50 for now\n",
    "        input: 256 x 256 x 3\n",
    "        output: 8 x 8 x 2048\n",
    "        '''\n",
    "        enc_model = ResNet50(include_top=False, weights='imagenet', input_tensor=inp)\n",
    "        \n",
    "        z_a = enc_model.output   # 8 x 8 x 2048\n",
    "        return z_a\n",
    "    \n",
    "    def pose_encoder(self, inp):\n",
    "        '''\n",
    "        reception / stacked hourglass\n",
    "        input: 256 x 256 x 3\n",
    "        output: [] x 8\n",
    "        '''\n",
    "        pose_model = PoseModel(inp, self.n_joints, self.n_blocks, self.reception_kernel_size).model\n",
    "        out = pose_model.output\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def concat(self, z_a, z_p):\n",
    "        '''\n",
    "        concat pose and appearance representations before decoding\n",
    "        input:\n",
    "            - z_p: \n",
    "            - z_a: 8 x 8 x 2048\n",
    "        output:\n",
    "        \n",
    "        TODO: This is where the real work should happen\n",
    "        '''\n",
    "        return z_a\n",
    "        \n",
    "    def decoder(self, concat):\n",
    "        '''\n",
    "        from concatenated representations to image reconstruction\n",
    "        input: 8 x 8 x 2048 (z_a)\n",
    "        output: 256 x 256 x 3\n",
    "        '''\n",
    "        decoder_model = DecoderModel(input_tensor=concat).model\n",
    "        out = decoder_model(concat)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_bce(y_true, y_pred):\n",
    "    '''\n",
    "    Elasticnet binary cross entropy for pose estimation\n",
    "    y_true\n",
    "    y_pred: (None, 16, 2)\n",
    "    '''\n",
    "    idx = tf.cast(tf.math.greater(y_true, 0.), tf.float32)\n",
    "    print(\"Shape %s\" % idx.shape)\n",
    "    #tmp_sum = tf.math.reduce_sum(idx, axis=(-1, -2))\n",
    "    #print(\"Shape sum %s\" % tmp_sum.shape)\n",
    "    #num_joints = tf.clip_by_value(tmp_sum, 1, None)\n",
    "    num_joints = y_pred.get_shape().as_list()[1]\n",
    "    print(\"Num joints %s\" % num_joints)\n",
    "\n",
    "    l1 = tf.math.abs(y_pred - y_true)\n",
    "    l2 = tf.math.square(y_pred - y_true)\n",
    "    bc = 0.01 * tf.keras.backend.binary_crossentropy(y_true, y_pred)  # doesn't expect logits like tf does\n",
    "    dummy = 0. * y_pred\n",
    "\n",
    "    return tf.reduce_sum(tf.where(tf.cast(idx, tf.bool), l1 + l2 + bc, dummy), axis=(-1, -2)) / num_joints\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_loss():\n",
    "    \n",
    "    def _multi_loss(y_true, y_pred):\n",
    "        print(\"y_true shape %s\" % (str(y_true.shape)))\n",
    "        print(\"y_pred shape %s\" % (str(y_pred.shape)))\n",
    "        \n",
    "        pose_loss = elastic_bce(p_true, p_pred)\n",
    "        return pose_loss\n",
    "    \n",
    "    return _multi_loss\n",
    "\n",
    "\n",
    "def pose_loss():\n",
    "    \n",
    "    def _pose_loss(y_true, y_pred):\n",
    "        print(\"pose y_true shape %s\" % (str(y_true.shape)))\n",
    "        print(\"pose y_pred shape %s\" % (str(y_pred.shape)))\n",
    "        \n",
    "        pose_loss = elastic_bce(y_true, y_pred)\n",
    "        return pose_loss\n",
    "    \n",
    "    return _pose_loss\n",
    "\n",
    "\n",
    "def reconstruction_loss():\n",
    "    \n",
    "    def _rec_loss(y_true, y_pred):\n",
    "        print(\"rec y_true shape %s\" % (str(y_true.shape)))\n",
    "        print(\"rec y_pred shape %s\" % (str(y_pred.shape)))\n",
    "        num_joints = y_pred.get_shape().as_list()[-1]\n",
    "        print(\"Num joints: %s\" % num_joints)\n",
    "        \n",
    "        rec_loss = tf.math.reduce_sum(tf.keras.backend.square(y_pred - y_true), axis=(-1, -2)) / num_joints\n",
    "        return rec_loss\n",
    "        \n",
    "    return _rec_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h36m_path = \"/share/DEEPLEARNING/datasets/human36m\"\n",
    "mpii_path = \"/share/DEEPLEARNING/datasets/mpii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h36m dataset loading\n",
    "h36m = Human36M(h36m_path, dataconf=config.human36m_dataconf, poselayout=pose_format.pa17j3d, topology='frames')\n",
    "\n",
    "data_tr = BatchLoader(\n",
    "    [h36m], \n",
    "    ['frame'], \n",
    "    ['pose'],\n",
    "    TRAIN_MODE, \n",
    "    batch_size=h36m.get_length(TRAIN_MODE),\n",
    "    num_predictions=num_predictions, \n",
    "    shuffle=True)\n",
    "\n",
    "# batch_size=[batch_size_mpii, batch_size_mpii, batch_size_ar, batch_size_ar], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "h36m_val = BatchLoader(\n",
    "    h36m, \n",
    "    ['frame'],\n",
    "    ['pose_w', 'pose_uvd', 'afmat', 'camera', 'action'], \n",
    "    VALID_MODE,\n",
    "    batch_size=h36m.get_length(VALID_MODE), \n",
    "    shuffle=True)\n",
    "\n",
    "[x_val], [pw_val, puvd_val, afmat_val, scam_val, action] = h36m_val[0]\n",
    "\n",
    "h36m_callback = H36MEvalCallback(x_val, pw_val, afmat_val, puvd_val[:,0,2], scam_val, action, logdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpii = MpiiSinglePerson(mpii_path, dataconf=config.mpii_dataconf, poselayout=pose_format.pa17j3d)\n",
    "mpii = MpiiSinglePerson(mpii_path, dataconf=config.mpii_dataconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr_mpii = BatchLoader(\n",
    "    mpii, \n",
    "    ['frame'], \n",
    "    ['frame', 'pose', 'pose', 'pose', 'pose'], \n",
    "    TRAIN_MODE,\n",
    "    batch_size=20,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_tr_mpii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data_tr_mpii.get_data(1, TRAIN_MODE)\n",
    "print(type(a), a.keys())\n",
    "print(a['pose'])\n",
    "print(\"pose shape %s\" % (str(a['pose'].shape)))\n",
    "print(\"frame shape %s\" % (str(a['frame'].shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data_tr_mpii[1]\n",
    "print(type(b), len(b))\n",
    "print(type(b[0]), len(b[0]))\n",
    "print(b[0][0].shape)\n",
    "print(type(b[1]), len(b[1]))\n",
    "print(b[1][0].shape, b[1][1].shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "img = b[0][0][0]\n",
    "print(img.shape)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder()\n",
    "model.build()\n",
    "\n",
    "# steps_per_epoch = h36m.get_length(TRAIN_MODE) // batch_size_h36m\n",
    "steps_per_epoch = mpii.get_length(TRAIN_MODE) // batch_size_mpii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data_tr, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AppearanceModel()\n",
    "model.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'appearance'\n",
    "dataset_name = 'mpii'\n",
    "model_folder = '/home/caleml/pe_experiments/exp_%s_%s_%s' % (model_name, dataset_name, datetime.datetime.now().strftime(\"%Y%m%d%H%M\")) \n",
    "os.makedirs(model_folder)\n",
    "model.train(data_tr_mpii, steps_per_epoch=len(data_tr_mpii), model_folder=model_folder, n_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiBranchModel(nb_pose_blocks=4)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multib'\n",
    "dataset_name = 'mpii'\n",
    "model_folder = '/home/caleml/pe_experiments/exp_%s_%s_%s' % (model_name, dataset_name, datetime.datetime.now().strftime(\"%Y%m%d%H%M\")) \n",
    "os.makedirs(model_folder)\n",
    "model.train(data_tr_mpii, steps_per_epoch=len(data_tr_mpii), model_folder=model_folder, n_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.models import AppearanceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = '/home/caleml/pe_experiments/exp_appearance_mpii_201902051901'\n",
    "model_checkpoint = '/home/caleml/pe_experiments/exp_appearance_mpii_201902051901/weights_mpii_058.h5'  # weights\n",
    "checkpoint_2 = '/home/caleml/pe_experiments/exp_appearance_mpii_201902061614/weights_mpii_013.h5'  # made with save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AppearanceModel()\n",
    "model.load(checkpoint_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval data\n",
    "mpii = MpiiSinglePerson(mpii_path, dataconf=config.mpii_dataconf, poselayout=pose_format.pa17j3d)\n",
    "data_val_mpii = BatchLoader(\n",
    "    mpii, \n",
    "    ['frame'], \n",
    "    ['frame'], \n",
    "    VALID_MODE,\n",
    "    shuffle=False)\n",
    "\n",
    "len(data_val_mpii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pouet(definition):\n",
    "    \n",
    "    ret = list()\n",
    "    for elt in definition:\n",
    "        ret.append('truc')\n",
    "        \n",
    "    return tuple(ret)\n",
    "\n",
    "a = pouet('a')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(i):\n",
    "    def loss(a):\n",
    "        return a * i\n",
    "    return loss\n",
    "\n",
    "losses = list()\n",
    "for i in range(10):\n",
    "    losses.append(wrapper(i))\n",
    "    \n",
    "print(len(losses))\n",
    "for loss_fn in losses:\n",
    "    print(loss_fn(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
